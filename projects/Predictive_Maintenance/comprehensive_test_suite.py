#!/usr/bin/env python3
"""
Comprehensive Test Suite for Predictive Maintenance System
Tests all scenarios with 5+ test cases each
"""

import requests
import time
import json
import random
from datetime import datetime, timedelta

API_BASE = "http://localhost:5001/api"

class PredictiveMaintenanceTestSuite:
    def __init__(self):
        self.test_results = {
            'dashboard_tests': [],
            'equipment_tests': [],
            'ml_training_tests': [],
            'realtime_tests': [],
            'api_tests': [],
            'integration_tests': []
        }
        self.passed_tests = 0
        self.total_tests = 0
    
    def log_test(self, category, test_name, passed, details=""):
        """Log test result"""
        self.test_results[category].append({
            'name': test_name,
            'passed': passed,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
        
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if details:
            print(f"    {details}")
    
    def test_dashboard_scenarios(self):
        """Test Dashboard functionality - 5+ scenarios"""
        print("\nüè† TESTING DASHBOARD SCENARIOS")
        print("=" * 50)
        
        # Test 1: Basic Dashboard Data Retrieval
        try:
            response = requests.get(f"{API_BASE}/dashboard")
            if response.status_code == 200:
                data = response.json()
                has_equipment = 'equipment' in data and len(data['equipment']) > 0
                has_metrics = 'metrics' in data
                self.log_test('dashboard_tests', 'Basic Dashboard Data Retrieval', 
                            has_equipment and has_metrics,
                            f"Equipment: {len(data.get('equipment', []))}, Metrics: {has_metrics}")
            else:
                self.log_test('dashboard_tests', 'Basic Dashboard Data Retrieval', False, 
                            f"HTTP {response.status_code}")
        except Exception as e:
            self.log_test('dashboard_tests', 'Basic Dashboard Data Retrieval', False, str(e))
        
        # Test 2: Equipment Health Score Validation
        try:
            response = requests.get(f"{API_BASE}/dashboard")
            if response.status_code == 200:
                data = response.json()
                equipment = data.get('equipment', [])
                valid_health_scores = all(
                    0 <= eq.get('healthScore', -1) <= 100 for eq in equipment
                )
                self.log_test('dashboard_tests', 'Equipment Health Score Validation', 
                            valid_health_scores,
                            f"All {len(equipment)} equipment have valid health scores (0-100%)")
            else:
                self.log_test('dashboard_tests', 'Equipment Health Score Validation', False)
        except Exception as e:
            self.log_test('dashboard_tests', 'Equipment Health Score Validation', False, str(e))
        
        # Test 3: Failure Probability Validation
        try:
            response = requests.get(f"{API_BASE}/dashboard")
            if response.status_code == 200:
                data = response.json()
                equipment = data.get('equipment', [])
                valid_failure_probs = all(
                    0 <= eq.get('failureProbability', -1) <= 100 for eq in equipment
                )
                self.log_test('dashboard_tests', 'Failure Probability Validation', 
                            valid_failure_probs,
                            f"All {len(equipment)} equipment have valid failure probabilities (0-100%)")
            else:
                self.log_test('dashboard_tests', 'Failure Probability Validation', False)
        except Exception as e:
            self.log_test('dashboard_tests', 'Failure Probability Validation', False, str(e))
        
        # Test 4: System Metrics Validation
        try:
            response = requests.get(f"{API_BASE}/dashboard")
            if response.status_code == 200:
                data = response.json()
                metrics = data.get('metrics', {})
                required_metrics = ['totalAlerts', 'activeAlerts', 'resolvedAlerts', 'avgUptime', 'predictionsToday']
                has_all_metrics = all(metric in metrics for metric in required_metrics)
                self.log_test('dashboard_tests', 'System Metrics Validation', 
                            has_all_metrics,
                            f"Metrics present: {list(metrics.keys())}")
            else:
                self.log_test('dashboard_tests', 'System Metrics Validation', False)
        except Exception as e:
            self.log_test('dashboard_tests', 'System Metrics Validation', False, str(e))
        
        # Test 5: Equipment Status Color Coding
        try:
            response = requests.get(f"{API_BASE}/dashboard")
            if response.status_code == 200:
                data = response.json()
                equipment = data.get('equipment', [])
                valid_statuses = all(
                    eq.get('status') in ['healthy', 'warning', 'critical'] for eq in equipment
                )
                self.log_test('dashboard_tests', 'Equipment Status Color Coding', 
                            valid_statuses,
                            f"All {len(equipment)} equipment have valid status values")
            else:
                self.log_test('dashboard_tests', 'Equipment Status Color Coding', False)
        except Exception as e:
            self.log_test('dashboard_tests', 'Equipment Status Color Coding', False, str(e))
        
        # Test 6: Dashboard Performance (Response Time)
        try:
            start_time = time.time()
            response = requests.get(f"{API_BASE}/dashboard")
            end_time = time.time()
            response_time = (end_time - start_time) * 1000  # Convert to ms
            
            fast_response = response_time < 1000  # Less than 1 second
            self.log_test('dashboard_tests', 'Dashboard Performance', 
                        fast_response and response.status_code == 200,
                        f"Response time: {response_time:.1f}ms")
        except Exception as e:
            self.log_test('dashboard_tests', 'Dashboard Performance', False, str(e))
    
    def test_equipment_detail_scenarios(self):
        """Test Equipment Detail functionality - 5+ scenarios"""
        print("\nüîß TESTING EQUIPMENT DETAIL SCENARIOS")
        print("=" * 50)
        
        # Test 1: Equipment Detail Data Retrieval
        try:
            response = requests.get(f"{API_BASE}/equipment/1")
            if response.status_code == 200:
                data = response.json()
                has_equipment = 'equipment' in data
                has_sensor_data = 'sensorData' in data
                self.log_test('equipment_tests', 'Equipment Detail Data Retrieval', 
                            has_equipment and has_sensor_data,
                            f"Equipment: {has_equipment}, Sensor Data: {has_sensor_data}")
            else:
                self.log_test('equipment_tests', 'Equipment Detail Data Retrieval', False)
        except Exception as e:
            self.log_test('equipment_tests', 'Equipment Detail Data Retrieval', False, str(e))
        
        # Test 2: Sensor Data Completeness
        try:
            response = requests.get(f"{API_BASE}/equipment/1")
            if response.status_code == 200:
                data = response.json()
                sensor_data = data.get('sensorData', {})
                expected_sensors = ['temperature', 'vibration', 'pressure', 'rpm']
                has_all_sensors = all(sensor in sensor_data for sensor in expected_sensors)
                sensor_counts = {sensor: len(data) for sensor, data in sensor_data.items()}
                self.log_test('equipment_tests', 'Sensor Data Completeness', 
                            has_all_sensors,
                            f"Sensors: {list(sensor_data.keys())}, Counts: {sensor_counts}")
            else:
                self.log_test('equipment_tests', 'Sensor Data Completeness', False)
        except Exception as e:
            self.log_test('equipment_tests', 'Sensor Data Completeness', False, str(e))
        
        # Test 3: Equipment Information Validation
        try:
            response = requests.get(f"{API_BASE}/equipment/1")
            if response.status_code == 200:
                data = response.json()
                equipment = data.get('equipment', {})
                required_fields = ['id', 'name', 'type', 'status', 'healthScore', 'failureProbability']
                has_all_fields = all(field in equipment for field in required_fields)
                self.log_test('equipment_tests', 'Equipment Information Validation', 
                            has_all_fields,
                            f"Fields present: {list(equipment.keys())}")
            else:
                self.log_test('equipment_tests', 'Equipment Information Validation', False)
        except Exception as e:
            self.log_test('equipment_tests', 'Equipment Information Validation', False, str(e))
        
        # Test 4: Multiple Equipment Access
        try:
            equipment_ids = [1, 2, 3]
            all_accessible = True
            for eq_id in equipment_ids:
                response = requests.get(f"{API_BASE}/equipment/{eq_id}")
                if response.status_code != 200:
                    all_accessible = False
                    break
            
            self.log_test('equipment_tests', 'Multiple Equipment Access', 
                        all_accessible,
                        f"All {len(equipment_ids)} equipment accessible")
        except Exception as e:
            self.log_test('equipment_tests', 'Multiple Equipment Access', False, str(e))
        
        # Test 5: Sensor Data Time Range
        try:
            response = requests.get(f"{API_BASE}/equipment/1")
            if response.status_code == 200:
                data = response.json()
                sensor_data = data.get('sensorData', {})
                
                # Check if sensor data has timestamps
                has_timestamps = True
                for sensor_type, sensor_points in sensor_data.items():
                    if sensor_points and 'timestamp' in sensor_points[0]:
                        has_timestamps = True
                        break
                
                self.log_test('equipment_tests', 'Sensor Data Time Range', 
                            has_timestamps,
                            f"Timestamps present in sensor data")
            else:
                self.log_test('equipment_tests', 'Sensor Data Time Range', False)
        except Exception as e:
            self.log_test('equipment_tests', 'Sensor Data Time Range', False, str(e))
        
        # Test 6: Equipment Detail Performance
        try:
            start_time = time.time()
            response = requests.get(f"{API_BASE}/equipment/1")
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            
            fast_response = response_time < 1000
            self.log_test('equipment_tests', 'Equipment Detail Performance', 
                        fast_response and response.status_code == 200,
                        f"Response time: {response_time:.1f}ms")
        except Exception as e:
            self.log_test('equipment_tests', 'Equipment Detail Performance', False, str(e))
    
    def test_ml_training_scenarios(self):
        """Test ML Training functionality - 5+ scenarios"""
        print("\nüß† TESTING ML TRAINING SCENARIOS")
        print("=" * 50)
        
        # Test 1: Initial Model Training
        try:
            response = requests.post(f"{API_BASE}/ml/train", json={"days_back": 7})
            if response.status_code == 200:
                data = response.json()
                training_success = data.get('success', False)
                has_performance = 'performance' in data
                self.log_test('ml_training_tests', 'Initial Model Training', 
                            training_success and has_performance,
                            f"Success: {training_success}, Performance data: {has_performance}")
            else:
                self.log_test('ml_training_tests', 'Initial Model Training', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Initial Model Training', False, str(e))
        
        # Test 2: Model Performance Validation
        try:
            response = requests.get(f"{API_BASE}/ml/performance")
            if response.status_code == 200:
                data = response.json()
                performance = data.get('performance', {})
                has_health_model = 'health_model' in performance
                has_failure_model = 'failure_model' in performance
                
                # Check for realistic performance metrics
                health_perf = performance.get('health_model', {})
                realistic_r2 = health_perf.get('r2', 0) < 0.99  # Not suspiciously high
                
                self.log_test('ml_training_tests', 'Model Performance Validation', 
                            has_health_model and has_failure_model and realistic_r2,
                            f"Health model: {has_health_model}, Failure model: {has_failure_model}, R¬≤: {health_perf.get('r2', 'N/A')}")
            else:
                self.log_test('ml_training_tests', 'Model Performance Validation', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Model Performance Validation', False, str(e))
        
        # Test 3: Incremental Retraining
        try:
            # Wait a bit to simulate new data
            time.sleep(2)
            response = requests.post(f"{API_BASE}/ml/retrain", json={"days_back": 7})
            if response.status_code == 200:
                data = response.json()
                retraining_success = data.get('success', False)
                retrained = data.get('retrained', False)
                self.log_test('ml_training_tests', 'Incremental Retraining', 
                            retraining_success,
                            f"Success: {retraining_success}, Retrained: {retrained}")
            else:
                self.log_test('ml_training_tests', 'Incremental Retraining', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Incremental Retraining', False, str(e))
        
        # Test 4: Model Validation Warnings
        try:
            response = requests.get(f"{API_BASE}/ml/status")
            if response.status_code == 200:
                data = response.json()
                warnings = data.get('warnings', [])
                validation_info = data.get('validation_info', {})
                
                has_warnings = len(warnings) > 0
                has_validation_info = len(validation_info) > 0
                
                self.log_test('ml_training_tests', 'Model Validation Warnings', 
                            has_warnings and has_validation_info,
                            f"Warnings: {len(warnings)}, Validation info: {has_validation_info}")
            else:
                self.log_test('ml_training_tests', 'Model Validation Warnings', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Model Validation Warnings', False, str(e))
        
        # Test 5: Cross-Validation Scores
        try:
            response = requests.get(f"{API_BASE}/ml/performance")
            if response.status_code == 200:
                data = response.json()
                performance = data.get('performance', {})
                
                health_perf = performance.get('health_model', {})
                failure_perf = performance.get('failure_model', {})
                
                has_cv_scores = (
                    'cv_score_mean' in health_perf and 'cv_score_std' in health_perf and
                    'cv_score_mean' in failure_perf and 'cv_score_std' in failure_perf
                )
                
                self.log_test('ml_training_tests', 'Cross-Validation Scores', 
                            has_cv_scores,
                            f"Health CV: {health_perf.get('cv_score_mean', 'N/A')}¬±{health_perf.get('cv_score_std', 'N/A')}")
            else:
                self.log_test('ml_training_tests', 'Cross-Validation Scores', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Cross-Validation Scores', False, str(e))
        
        # Test 6: Training Data Sufficiency
        try:
            response = requests.get(f"{API_BASE}/ml/performance")
            if response.status_code == 200:
                data = response.json()
                performance = data.get('performance', {})
                
                health_perf = performance.get('health_model', {})
                failure_perf = performance.get('failure_model', {})
                
                sufficient_samples = (
                    health_perf.get('training_samples', 0) >= 100 and
                    failure_perf.get('training_samples', 0) >= 100
                )
                
                self.log_test('ml_training_tests', 'Training Data Sufficiency', 
                            sufficient_samples,
                            f"Health samples: {health_perf.get('training_samples', 0)}, Failure samples: {failure_perf.get('training_samples', 0)}")
            else:
                self.log_test('ml_training_tests', 'Training Data Sufficiency', False)
        except Exception as e:
            self.log_test('ml_training_tests', 'Training Data Sufficiency', False, str(e))
    
    def test_realtime_scenarios(self):
        """Test Real-time Updates functionality - 5+ scenarios"""
        print("\n‚ö° TESTING REAL-TIME UPDATE SCENARIOS")
        print("=" * 50)
        
        # Test 1: Dashboard Real-time Updates
        try:
            responses = []
            for i in range(3):
                response = requests.get(f"{API_BASE}/dashboard")
                responses.append(response)
                time.sleep(1)  # Wait 1 second between requests
            
            all_successful = all(r.status_code == 200 for r in responses)
            self.log_test('realtime_tests', 'Dashboard Real-time Updates', 
                        all_successful,
                        f"All {len(responses)} requests successful")
        except Exception as e:
            self.log_test('realtime_tests', 'Dashboard Real-time Updates', False, str(e))
        
        # Test 2: Equipment Detail Real-time Updates
        try:
            responses = []
            for i in range(3):
                response = requests.get(f"{API_BASE}/equipment/1")
                responses.append(response)
                time.sleep(1)
            
            all_successful = all(r.status_code == 200 for r in responses)
            self.log_test('realtime_tests', 'Equipment Detail Real-time Updates', 
                        all_successful,
                        f"All {len(responses)} requests successful")
        except Exception as e:
            self.log_test('realtime_tests', 'Equipment Detail Real-time Updates', False, str(e))
        
        # Test 3: Data Consistency Over Time
        try:
            response1 = requests.get(f"{API_BASE}/dashboard")
            time.sleep(2)
            response2 = requests.get(f"{API_BASE}/dashboard")
            
            if response1.status_code == 200 and response2.status_code == 200:
                data1 = response1.json()
                data2 = response2.json()
                
                # Check if data structure is consistent
                consistent_structure = (
                    'equipment' in data1 and 'equipment' in data2 and
                    'metrics' in data1 and 'metrics' in data2
                )
                
                self.log_test('realtime_tests', 'Data Consistency Over Time', 
                            consistent_structure,
                            f"Structure consistent: {consistent_structure}")
            else:
                self.log_test('realtime_tests', 'Data Consistency Over Time', False)
        except Exception as e:
            self.log_test('realtime_tests', 'Data Consistency Over Time', False, str(e))
        
        # Test 4: Simulator Status Monitoring
        try:
            response = requests.get(f"{API_BASE}/simulator/status")
            if response.status_code == 200:
                data = response.json()
                has_running_status = 'running' in data
                self.log_test('realtime_tests', 'Simulator Status Monitoring', 
                            has_running_status,
                            f"Simulator running: {data.get('running', False)}")
            else:
                self.log_test('realtime_tests', 'Simulator Status Monitoring', False)
        except Exception as e:
            self.log_test('realtime_tests', 'Simulator Status Monitoring', False, str(e))
        
        # Test 5: ML Status Real-time Updates
        try:
            responses = []
            for i in range(3):
                response = requests.get(f"{API_BASE}/ml/status")
                responses.append(response)
                time.sleep(1)
            
            all_successful = all(r.status_code == 200 for r in responses)
            self.log_test('realtime_tests', 'ML Status Real-time Updates', 
                        all_successful,
                        f"All {len(responses)} ML status requests successful")
        except Exception as e:
            self.log_test('realtime_tests', 'ML Status Real-time Updates', False, str(e))
        
        # Test 6: Performance Under Load
        try:
            start_time = time.time()
            responses = []
            for i in range(10):  # 10 concurrent requests
                response = requests.get(f"{API_BASE}/dashboard")
                responses.append(response)
            
            end_time = time.time()
            total_time = (end_time - start_time) * 1000
            
            successful_requests = sum(1 for r in responses if r.status_code == 200)
            performance_acceptable = total_time < 5000 and successful_requests >= 8  # 80% success rate
            
            self.log_test('realtime_tests', 'Performance Under Load', 
                        performance_acceptable,
                        f"Success rate: {successful_requests}/10, Total time: {total_time:.1f}ms")
        except Exception as e:
            self.log_test('realtime_tests', 'Performance Under Load', False, str(e))
    
    def test_api_scenarios(self):
        """Test API Endpoints functionality - 5+ scenarios"""
        print("\nüåê TESTING API ENDPOINT SCENARIOS")
        print("=" * 50)
        
        # Test 1: All Core Endpoints Accessibility
        try:
            endpoints = [
                f"{API_BASE}/dashboard",
                f"{API_BASE}/equipment/1",
                f"{API_BASE}/alerts",
                f"{API_BASE}/ml/status",
                f"{API_BASE}/ml/performance",
                f"{API_BASE}/simulator/status"
            ]
            
            accessible_endpoints = 0
            for endpoint in endpoints:
                response = requests.get(endpoint)
                if response.status_code == 200:
                    accessible_endpoints += 1
            
            all_accessible = accessible_endpoints == len(endpoints)
            self.log_test('api_tests', 'All Core Endpoints Accessibility', 
                        all_accessible,
                        f"{accessible_endpoints}/{len(endpoints)} endpoints accessible")
        except Exception as e:
            self.log_test('api_tests', 'All Core Endpoints Accessibility', False, str(e))
        
        # Test 2: POST Endpoints Functionality
        try:
            post_endpoints = [
                (f"{API_BASE}/ml/train", {"days_back": 7}),
                (f"{API_BASE}/ml/retrain", {"days_back": 7}),
                (f"{API_BASE}/ml/predict", {"sensor_data": {"temperature": 75, "vibration": 2, "pressure": 20, "rpm": 1800}})
            ]
            
            successful_posts = 0
            for endpoint, data in post_endpoints:
                response = requests.post(endpoint, json=data)
                if response.status_code == 200:
                    successful_posts += 1
            
            all_posts_work = successful_posts == len(post_endpoints)
            self.log_test('api_tests', 'POST Endpoints Functionality', 
                        all_posts_work,
                        f"{successful_posts}/{len(post_endpoints)} POST endpoints working")
        except Exception as e:
            self.log_test('api_tests', 'POST Endpoints Functionality', False, str(e))
        
        # Test 3: Error Handling
        try:
            # Test invalid equipment ID
            response = requests.get(f"{API_BASE}/equipment/999")
            handles_invalid_id = response.status_code in [404, 400] or response.status_code == 200
            
            # Test invalid ML prediction data
            response = requests.post(f"{API_BASE}/ml/predict", json={"invalid": "data"})
            handles_invalid_data = response.status_code in [400, 422]
            
            error_handling_works = handles_invalid_id and handles_invalid_data
            self.log_test('api_tests', 'Error Handling', 
                        error_handling_works,
                        f"Invalid ID: {handles_invalid_id}, Invalid data: {handles_invalid_data}")
        except Exception as e:
            self.log_test('api_tests', 'Error Handling', False, str(e))
        
        # Test 4: Response Format Consistency
        try:
            endpoints = [
                f"{API_BASE}/dashboard",
                f"{API_BASE}/equipment/1",
                f"{API_BASE}/ml/status"
            ]
            
            consistent_format = True
            for endpoint in endpoints:
                response = requests.get(endpoint)
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if not isinstance(data, dict):
                            consistent_format = False
                            break
                    except:
                        consistent_format = False
                        break
            
            self.log_test('api_tests', 'Response Format Consistency', 
                        consistent_format,
                        f"All responses are valid JSON objects")
        except Exception as e:
            self.log_test('api_tests', 'Response Format Consistency', False, str(e))
        
        # Test 5: API Performance
        try:
            endpoints = [
                f"{API_BASE}/dashboard",
                f"{API_BASE}/equipment/1",
                f"{API_BASE}/ml/status"
            ]
            
            fast_responses = 0
            for endpoint in endpoints:
                start_time = time.time()
                response = requests.get(endpoint)
                end_time = time.time()
                response_time = (end_time - start_time) * 1000
                
                if response_time < 1000:  # Less than 1 second
                    fast_responses += 1
            
            performance_good = fast_responses == len(endpoints)
            self.log_test('api_tests', 'API Performance', 
                        performance_good,
                        f"{fast_responses}/{len(endpoints)} endpoints respond in <1s")
        except Exception as e:
            self.log_test('api_tests', 'API Performance', False, str(e))
        
        # Test 6: CORS Headers
        try:
            response = requests.options(f"{API_BASE}/dashboard")
            has_cors_headers = 'Access-Control-Allow-Origin' in response.headers
            self.log_test('api_tests', 'CORS Headers', 
                        has_cors_headers,
                        f"CORS headers present: {has_cors_headers}")
        except Exception as e:
            self.log_test('api_tests', 'CORS Headers', False, str(e))
    
    def test_integration_scenarios(self):
        """Test Integration scenarios - 5+ scenarios"""
        print("\nüîó TESTING INTEGRATION SCENARIOS")
        print("=" * 50)
        
        # Test 1: End-to-End Workflow
        try:
            # 1. Get dashboard data
            dashboard_response = requests.get(f"{API_BASE}/dashboard")
            dashboard_success = dashboard_response.status_code == 200
            
            # 2. Get equipment detail
            equipment_response = requests.get(f"{API_BASE}/equipment/1")
            equipment_success = equipment_response.status_code == 200
            
            # 3. Train ML models
            ml_response = requests.post(f"{API_BASE}/ml/train", json={"days_back": 7})
            ml_success = ml_response.status_code == 200
            
            # 4. Make prediction
            prediction_response = requests.post(f"{API_BASE}/ml/predict", 
                                             json={"sensor_data": {"temperature": 75, "vibration": 2, "pressure": 20, "rpm": 1800}})
            prediction_success = prediction_response.status_code == 200
            
            end_to_end_success = all([dashboard_success, equipment_success, ml_success, prediction_success])
            self.log_test('integration_tests', 'End-to-End Workflow', 
                        end_to_end_success,
                        f"Dashboard: {dashboard_success}, Equipment: {equipment_success}, ML: {ml_success}, Prediction: {prediction_success}")
        except Exception as e:
            self.log_test('integration_tests', 'End-to-End Workflow', False, str(e))
        
        # Test 2: Data Flow Consistency
        try:
            # Get dashboard and equipment data
            dashboard_response = requests.get(f"{API_BASE}/dashboard")
            equipment_response = requests.get(f"{API_BASE}/equipment/1")
            
            if dashboard_response.status_code == 200 and equipment_response.status_code == 200:
                dashboard_data = dashboard_response.json()
                equipment_data = equipment_response.json()
                
                # Check if equipment data is consistent
                dashboard_equipment = dashboard_data.get('equipment', [])
                equipment_detail = equipment_data.get('equipment', {})
                
                # Find matching equipment
                matching_equipment = None
                for eq in dashboard_equipment:
                    if eq.get('id') == equipment_detail.get('id'):
                        matching_equipment = eq
                        break
                
                data_consistent = (
                    matching_equipment is not None and
                    abs(matching_equipment.get('healthScore', 0) - equipment_detail.get('healthScore', 0)) < 0.1
                )
                
                self.log_test('integration_tests', 'Data Flow Consistency', 
                            data_consistent,
                            f"Equipment data consistent between dashboard and detail views")
            else:
                self.log_test('integration_tests', 'Data Flow Consistency', False)
        except Exception as e:
            self.log_test('integration_tests', 'Data Flow Consistency', False, str(e))
        
        # Test 3: ML Integration with Equipment Data
        try:
            # Get equipment data
            equipment_response = requests.get(f"{API_BASE}/equipment/1")
            if equipment_response.status_code == 200:
                equipment_data = equipment_response.json()
                sensor_data = equipment_data.get('sensorData', {})
                
                # Use actual sensor data for prediction
                if sensor_data and 'temperature' in sensor_data:
                    latest_temp = sensor_data['temperature'][0] if sensor_data['temperature'] else 75
                    latest_vibration = sensor_data['vibration'][0] if sensor_data['vibration'] else 2
                    latest_pressure = sensor_data['pressure'][0] if sensor_data['pressure'] else 20
                    latest_rpm = sensor_data['rpm'][0] if sensor_data['rpm'] else 1800
                    
                    prediction_response = requests.post(f"{API_BASE}/ml/predict", 
                                                     json={"sensor_data": {
                                                         "temperature": latest_temp,
                                                         "vibration": latest_vibration,
                                                         "pressure": latest_pressure,
                                                         "rpm": latest_rpm
                                                     }})
                    
                    ml_integration_success = prediction_response.status_code == 200
                    self.log_test('integration_tests', 'ML Integration with Equipment Data', 
                                ml_integration_success,
                                f"Prediction using real sensor data: {ml_integration_success}")
                else:
                    self.log_test('integration_tests', 'ML Integration with Equipment Data', False, "No sensor data available")
            else:
                self.log_test('integration_tests', 'ML Integration with Equipment Data', False)
        except Exception as e:
            self.log_test('integration_tests', 'ML Integration with Equipment Data', False, str(e))
        
        # Test 4: Real-time Data Integration
        try:
            # Get data multiple times to check for updates
            responses = []
            for i in range(3):
                dashboard_response = requests.get(f"{API_BASE}/dashboard")
                equipment_response = requests.get(f"{API_BASE}/equipment/1")
                responses.append((dashboard_response, equipment_response))
                time.sleep(2)
            
            all_successful = all(
                dashboard.status_code == 200 and equipment.status_code == 200 
                for dashboard, equipment in responses
            )
            
            self.log_test('integration_tests', 'Real-time Data Integration', 
                        all_successful,
                        f"All {len(responses)} data collection cycles successful")
        except Exception as e:
            self.log_test('integration_tests', 'Real-time Data Integration', False, str(e))
        
        # Test 5: System Health Monitoring
        try:
            # Check all system components
            components = {
                'Dashboard': requests.get(f"{API_BASE}/dashboard").status_code == 200,
                'Equipment': requests.get(f"{API_BASE}/equipment/1").status_code == 200,
                'Alerts': requests.get(f"{API_BASE}/alerts").status_code == 200,
                'ML Status': requests.get(f"{API_BASE}/ml/status").status_code == 200,
                'Simulator': requests.get(f"{API_BASE}/simulator/status").status_code == 200
            }
            
            healthy_components = sum(components.values())
            system_healthy = healthy_components >= 4  # At least 4/5 components healthy
            
            self.log_test('integration_tests', 'System Health Monitoring', 
                        system_healthy,
                        f"Healthy components: {healthy_components}/5 - {dict(components)}")
        except Exception as e:
            self.log_test('integration_tests', 'System Health Monitoring', False, str(e))
        
        # Test 6: Performance Under Continuous Load
        try:
            start_time = time.time()
            successful_requests = 0
            
            # Run continuous requests for 10 seconds
            while (time.time() - start_time) < 10:
                dashboard_response = requests.get(f"{API_BASE}/dashboard")
                if dashboard_response.status_code == 200:
                    successful_requests += 1
                time.sleep(0.5)  # Request every 500ms
            
            total_time = time.time() - start_time
            requests_per_second = successful_requests / total_time
            performance_acceptable = requests_per_second >= 1.0  # At least 1 request per second
            
            self.log_test('integration_tests', 'Performance Under Continuous Load', 
                        performance_acceptable,
                        f"Requests/sec: {requests_per_second:.2f}, Total requests: {successful_requests}")
        except Exception as e:
            self.log_test('integration_tests', 'Performance Under Continuous Load', False, str(e))
    
    def run_all_tests(self):
        """Run all test scenarios"""
        print("üéØ COMPREHENSIVE PREDICTIVE MAINTENANCE TEST SUITE")
        print("=" * 70)
        print("Testing all scenarios with 5+ test cases each")
        print("=" * 70)
        
        # Run all test categories
        self.test_dashboard_scenarios()
        self.test_equipment_detail_scenarios()
        self.test_ml_training_scenarios()
        self.test_realtime_scenarios()
        self.test_api_scenarios()
        self.test_integration_scenarios()
        
        # Generate summary report
        self.generate_summary_report()
    
    def generate_summary_report(self):
        """Generate comprehensive test summary report"""
        print("\n" + "=" * 70)
        print("üìä COMPREHENSIVE TEST SUMMARY REPORT")
        print("=" * 70)
        
        # Overall statistics
        success_rate = (self.passed_tests / self.total_tests) * 100 if self.total_tests > 0 else 0
        
        print(f"üìà Overall Success Rate: {success_rate:.1f}% ({self.passed_tests}/{self.total_tests})")
        print()
        
        # Category-wise results
        for category, tests in self.test_results.items():
            if tests:
                passed = sum(1 for test in tests if test['passed'])
                total = len(tests)
                category_rate = (passed / total) * 100 if total > 0 else 0
                
                print(f"üìã {category.replace('_', ' ').title()}: {category_rate:.1f}% ({passed}/{total})")
                
                # Show failed tests
                failed_tests = [test for test in tests if not test['passed']]
                if failed_tests:
                    print("   ‚ùå Failed Tests:")
                    for test in failed_tests:
                        print(f"      ‚Ä¢ {test['name']}: {test['details']}")
                print()
        
        # System readiness assessment
        if success_rate >= 90:
            print("üéâ SYSTEM READY FOR HACKATHON DEMO!")
            print("‚úÖ All critical functionality working")
            print("‚úÖ Performance meets requirements")
            print("‚úÖ Integration tests passing")
        elif success_rate >= 75:
            print("‚ö†Ô∏è  SYSTEM MOSTLY READY")
            print("‚úÖ Core functionality working")
            print("‚ö†Ô∏è  Some minor issues to address")
        else:
            print("‚ùå SYSTEM NEEDS ATTENTION")
            print("‚ùå Critical issues detected")
            print("üîß Review failed tests above")
        
        print("\nüöÄ Ready for hackathon demonstration!")

def main():
    """Run the comprehensive test suite"""
    test_suite = PredictiveMaintenanceTestSuite()
    test_suite.run_all_tests()

if __name__ == "__main__":
    main()
